{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e64b2b",
   "metadata": {},
   "source": [
    "#### This is a script to extract the simulated data using PGS based regression model to estimate indirect genetic effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18e94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# a function to read the .txt file, select columns based on the focal trait, and return a DataFrame\n",
    "# the last character focal trait colname names are 1 or 2 in the data, use those to filter\n",
    "def read_trait_data(file_path, focal_trait='both', combine_pgs=False):\n",
    "    \"\"\"\n",
    "    Read trait data from a file and extract specific traits.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the data file\n",
    "    focal_trait (str or int): Which trait(s) to extract\n",
    "                              - 'trait1' or 1: Extract only trait 1 columns (ending with '1')\n",
    "                              - 'trait2' or 2: Extract only trait 2 columns (ending with '2')  \n",
    "                              - 'both' or 'all': Extract both trait 1 and trait 2 columns\n",
    "    combine_pgs (bool): Whether to combine haplotypic PGS scores into full PGS scores\n",
    "                        - True: Combine NTp+Tp->PGSp, NTm+Tm->PGSm, Tp+Tm->PGSo\n",
    "                        - False: Keep original columns separate\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the selected trait columns (and combined PGS if requested)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the data file\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Get all column names\n",
    "    all_columns = df.columns.tolist()\n",
    "    \n",
    "    # Convert focal_trait to string for consistent handling\n",
    "    if focal_trait == 1:\n",
    "        focal_trait = 'trait1'\n",
    "    elif focal_trait == 2:\n",
    "        focal_trait = 'trait2'\n",
    "    \n",
    "    # Select columns based on focal_trait\n",
    "    if focal_trait.lower() in ['trait1', '1']:\n",
    "        # Select columns ending with '1'\n",
    "        selected_columns = [col for col in all_columns if col.endswith('1')]\n",
    "        print(f\"Selected trait 1 columns: {selected_columns}\")\n",
    "        \n",
    "    elif focal_trait.lower() in ['trait2', '2']:\n",
    "        # Select columns ending with '2'\n",
    "        selected_columns = [col for col in all_columns if col.endswith('2')]\n",
    "        print(f\"Selected trait 2 columns: {selected_columns}\")\n",
    "        \n",
    "    elif focal_trait.lower() in ['both', 'all']:\n",
    "        # Select all columns (both trait 1 and trait 2)\n",
    "        selected_columns = all_columns\n",
    "        print(f\"Selected all columns: {len(selected_columns)} columns\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Invalid focal_trait: {focal_trait}. Use 'trait1', 'trait2', or 'both'\")\n",
    "        return None\n",
    "    \n",
    "    # Get the DataFrame with selected columns\n",
    "    result_df = df[selected_columns].copy()\n",
    "    \n",
    "    # Combine haplotypic PGS scores if requested\n",
    "    if combine_pgs:\n",
    "        print(\"Combining haplotypic PGS scores into full PGS scores...\")\n",
    "        \n",
    "        # Helper function to combine PGS for a specific trait\n",
    "        def combine_trait_pgs(df, trait_suffix):\n",
    "            trait_cols = {}\n",
    "            \n",
    "            # Check if the required columns exist for this trait\n",
    "            nt_col = f'NT{trait_suffix}'  # Non-transmitted PGS\n",
    "            t_col = f'T{trait_suffix}'    # Transmitted PGS\n",
    "            \n",
    "            if nt_col in df.columns and t_col in df.columns:\n",
    "                # Combine NTp + Tp -> PGSp (or NTm + Tm -> PGSm)\n",
    "                pgs_col = f'PGS{trait_suffix}'\n",
    "                df[pgs_col] = df[nt_col] + df[t_col]\n",
    "                trait_cols[pgs_col] = f\"Combined {nt_col} + {t_col}\"\n",
    "                print(f\"  Created {pgs_col} = {nt_col} + {t_col}\")\n",
    "            \n",
    "            return trait_cols\n",
    "        \n",
    "        # Combine PGS for paternal (p) and maternal (m) scores\n",
    "        combined_cols = {}\n",
    "        \n",
    "        # For paternal PGS (NTp + Tp -> PGSp)\n",
    "        if focal_trait.lower() in ['trait1', '1', 'both', 'all']:\n",
    "            combined_cols.update(combine_trait_pgs(result_df, 'p1'))\n",
    "        if focal_trait.lower() in ['trait2', '2', 'both', 'all']:\n",
    "            combined_cols.update(combine_trait_pgs(result_df, 'p2'))\n",
    "            \n",
    "        # For maternal PGS (NTm + Tm -> PGSm)  \n",
    "        if focal_trait.lower() in ['trait1', '1', 'both', 'all']:\n",
    "            combined_cols.update(combine_trait_pgs(result_df, 'm1'))\n",
    "        if focal_trait.lower() in ['trait2', '2', 'both', 'all']:\n",
    "            combined_cols.update(combine_trait_pgs(result_df, 'm2'))\n",
    "        \n",
    "        # For offspring PGS (Tp + Tm -> PGSo) - combines parental transmitted alleles\n",
    "        if focal_trait.lower() in ['trait1', '1', 'both', 'all']:\n",
    "            if 'Tp1' in result_df.columns and 'Tm1' in result_df.columns:\n",
    "                result_df['PGSo1'] = result_df['Tp1'] + result_df['Tm1']\n",
    "                combined_cols['PGSo1'] = \"Combined Tp1 + Tm1\"\n",
    "                print(f\"  Created PGSo1 = Tp1 + Tm1\")\n",
    "                \n",
    "        if focal_trait.lower() in ['trait2', '2', 'both', 'all']:\n",
    "            if 'Tp2' in result_df.columns and 'Tm2' in result_df.columns:\n",
    "                result_df['PGSo2'] = result_df['Tp2'] + result_df['Tm2']\n",
    "                combined_cols['PGSo2'] = \"Combined Tp2 + Tm2\"\n",
    "                print(f\"  Created PGSo2 = Tp2 + Tm2\")\n",
    "        \n",
    "        if combined_cols:\n",
    "            print(f\"Successfully created {len(combined_cols)} combined PGS columns\")\n",
    "        else:\n",
    "            print(\"No PGS columns were combined (missing required T/NT columns)\")\n",
    "    \n",
    "    # Return the DataFrame with selected columns (and combined PGS if requested)\n",
    "    return result_df\n",
    "\n",
    "# Example usage function to demonstrate how to use read_trait_data\n",
    "def load_data_example():\n",
    "    \"\"\"\n",
    "    Example function showing how to use read_trait_data with different parameters\n",
    "    \"\"\"\n",
    "    # Example file path (adjust as needed)\n",
    "    file_path = \"/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Data/phenoVT_geneticAM/nfam8000/phenoVT_geneticAM_run_001_nfam8000.txt\"\n",
    "    \n",
    "    print(\"=== Loading trait 1 only ===\")\n",
    "    trait1_data = read_trait_data(file_path, focal_trait='trait1')\n",
    "    if trait1_data is not None:\n",
    "        print(f\"Trait 1 data shape: {trait1_data.shape}\")\n",
    "        print(trait1_data.head(3))\n",
    "    \n",
    "    print(\"\\n=== Loading trait 2 only ===\")\n",
    "    trait2_data = read_trait_data(file_path, focal_trait='trait2')\n",
    "    if trait2_data is not None:\n",
    "        print(f\"Trait 2 data shape: {trait2_data.shape}\")\n",
    "        print(trait2_data.head(3))\n",
    "    \n",
    "    print(\"\\n=== Loading both traits ===\")\n",
    "    both_traits_data = read_trait_data(file_path, focal_trait='both')\n",
    "    if both_traits_data is not None:\n",
    "        print(f\"Both traits data shape: {both_traits_data.shape}\")\n",
    "        print(both_traits_data.head(3))\n",
    "    \n",
    "    print(\"\\n=== Loading both traits with combined PGS ===\")\n",
    "    combined_pgs_data = read_trait_data(file_path, focal_trait='both', combine_pgs=True)\n",
    "    if combined_pgs_data is not None:\n",
    "        print(f\"Combined PGS data shape: {combined_pgs_data.shape}\")\n",
    "        print(\"Available columns:\", combined_pgs_data.columns.tolist())\n",
    "        print(combined_pgs_data.head(3))\n",
    "    \n",
    "    return trait1_data, trait2_data, both_traits_data, combined_pgs_data\n",
    "\n",
    "# a function to loop through the directory and run a specified regression on using each df\n",
    "def run_analysis_on_directory(directory_path, analysis_function, \n",
    "                             focal_trait='both', combine_pgs=False, \n",
    "                             file_pattern='*.txt', save_results=True, \n",
    "                             output_dir=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Generic function to loop through files in a directory and run any analysis on each file.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (str): Path to the directory containing data files\n",
    "    analysis_function (callable): Function to run analysis on each DataFrame\n",
    "                                 Should accept DataFrame as first argument and filename as second\n",
    "                                 Example: my_analysis(dataframe, filename, **other_params)\n",
    "    focal_trait (str): Which trait(s) to extract ('trait1', 'trait2', or 'both')\n",
    "    combine_pgs (bool): Whether to combine haplotypic PGS scores\n",
    "    file_pattern (str): Pattern to match files (default: '*.txt')\n",
    "    save_results (bool): Whether to save results to files\n",
    "    output_dir (str): Directory to save results (if None, uses directory_path)\n",
    "    **kwargs: Additional keyword arguments to pass to the analysis_function\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with filename as key and analysis results as value\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    \n",
    "    # Get all files matching the pattern\n",
    "    file_pattern_full = os.path.join(directory_path, file_pattern)\n",
    "    files = glob.glob(file_pattern_full)\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No files found matching pattern: {file_pattern_full}\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Found {len(files)} files to process\")\n",
    "    \n",
    "    # Set output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = directory_path\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    results = {}\n",
    "    failed_files = []\n",
    "    \n",
    "    for i, file_path in enumerate(files, 1):\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"\\nProcessing file {i}/{len(files)}: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Load data using our read_trait_data function\n",
    "            data = read_trait_data(file_path, focal_trait=focal_trait, combine_pgs=combine_pgs)\n",
    "            \n",
    "            if data is None:\n",
    "                print(f\"  Failed to load data from {filename}\")\n",
    "                failed_files.append(filename)\n",
    "                continue\n",
    "            \n",
    "            # Run analysis\n",
    "            print(f\"  Running analysis on {data.shape[0]} rows, {data.shape[1]} columns\")\n",
    "            analysis_result = analysis_function(data, filename=filename, **kwargs)\n",
    "            \n",
    "            # Store results\n",
    "            results[filename] = analysis_result\n",
    "            \n",
    "            # Save results if requested\n",
    "            if save_results and analysis_result is not None:\n",
    "                output_file = os.path.join(output_dir, f\"analysis_results_{filename.replace('.txt', '.csv')}\")\n",
    "                \n",
    "                # Handle different types of analysis results\n",
    "                if isinstance(analysis_result, pd.DataFrame):\n",
    "                    analysis_result.to_csv(output_file, index=False)\n",
    "                    print(f\"  Saved results to: {output_file}\")\n",
    "                elif isinstance(analysis_result, dict):\n",
    "                    # Convert dict to DataFrame for saving\n",
    "                    pd.DataFrame([analysis_result]).to_csv(output_file, index=False)\n",
    "                    print(f\"  Saved results to: {output_file}\")\n",
    "                else:\n",
    "                    print(f\"  Warning: Results type {type(analysis_result)} not supported for saving\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {filename}: {str(e)}\")\n",
    "            failed_files.append(filename)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n=== Processing Summary ===\")\n",
    "    print(f\"Total files processed: {len(files)}\")\n",
    "    print(f\"Successfully processed: {len(results)}\")\n",
    "    print(f\"Failed: {len(failed_files)}\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"Failed files: {failed_files}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example analysis functions that can be used with run_analysis_on_directory\n",
    "def basic_stats_analysis(data, filename=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Example analysis function: Basic descriptive statistics\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The data to analyze\n",
    "    filename (str): Optional filename for reference\n",
    "    **kwargs: Additional parameters (ignored in this example)\n",
    "    \n",
    "    Returns:\n",
    "    dict: Basic statistics results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'filename': filename,\n",
    "        'n_rows': len(data),\n",
    "        'n_columns': len(data.columns),\n",
    "        'columns': list(data.columns)\n",
    "    }\n",
    "    \n",
    "    # Add basic stats for numeric columns\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        stats = data[numeric_cols].describe()\n",
    "        for col in numeric_cols:\n",
    "            results[f'{col}_mean'] = stats.loc['mean', col]\n",
    "            results[f'{col}_std'] = stats.loc['std', col]\n",
    "            results[f'{col}_min'] = stats.loc['min', col]\n",
    "            results[f'{col}_max'] = stats.loc['max', col]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def custom_regression_analysis(data, filename=None, predictors=None, outcomes=None, \n",
    "                             multiple_regression=True, incremental_r2=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Generic regression analysis function that supports both simple and multiple regression\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The data to analyze\n",
    "    filename (str): Optional filename for reference  \n",
    "    predictors (list): List of predictor column names\n",
    "    outcomes (list): List of outcome column names\n",
    "    multiple_regression (bool): If True and len(predictors)>1, run multiple regression\n",
    "                               If False, run simple regression for each predictor-outcome pair\n",
    "    incremental_r2 (bool): If True and multiple_regression=True, calculate incremental R²\n",
    "                          Shows R² contribution of each predictor when added sequentially\n",
    "    **kwargs: Additional parameters for regression\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Results from regression analyses\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import r2_score\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    # Default predictors and outcomes if not specified\n",
    "    if predictors is None:\n",
    "        predictors = [col for col in data.columns if 'PGS' in col or 'T' in col]\n",
    "    if outcomes is None:\n",
    "        outcomes = [col for col in data.columns if col.startswith('Y')]\n",
    "    \n",
    "    # Ensure predictors and outcomes are lists\n",
    "    if isinstance(predictors, str):\n",
    "        predictors = [predictors]\n",
    "    if isinstance(outcomes, str):\n",
    "        outcomes = [outcomes]\n",
    "    \n",
    "    # Check if we should run multiple regression\n",
    "    if multiple_regression and len(predictors) > 1:\n",
    "        print(f\"  Running multiple regression with {len(predictors)} predictors\")\n",
    "        \n",
    "        # Run multiple regression for each outcome\n",
    "        for outcome in outcomes:\n",
    "            if outcome in data.columns:\n",
    "                try:\n",
    "                    # Check which predictors are available\n",
    "                    available_predictors = [p for p in predictors if p in data.columns]\n",
    "                    \n",
    "                    if len(available_predictors) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Prepare data for multiple regression\n",
    "                    X = data[available_predictors].values\n",
    "                    y = data[outcome].values\n",
    "                    \n",
    "                    # Remove any NaN values\n",
    "                    mask = ~(pd.isna(X).any(axis=1) | pd.isna(y))\n",
    "                    X_clean = X[mask]\n",
    "                    y_clean = y[mask]\n",
    "                    \n",
    "                    if len(X_clean) > len(available_predictors) + 5:  # Need more samples than predictors\n",
    "                        \n",
    "                        if incremental_r2:\n",
    "                            print(f\"    Calculating incremental R² for {len(available_predictors)} predictors\")\n",
    "                            \n",
    "                            # Calculate incremental R² by adding predictors sequentially\n",
    "                            incremental_results = []\n",
    "                            previous_r2 = 0\n",
    "                            \n",
    "                            for i in range(1, len(available_predictors) + 1):\n",
    "                                # Use first i predictors\n",
    "                                current_predictors = available_predictors[:i]\n",
    "                                X_current = X_clean[:, :i]\n",
    "                                \n",
    "                                # Fit model with current predictors\n",
    "                                model_current = LinearRegression()\n",
    "                                model_current.fit(X_current, y_clean)\n",
    "                                y_pred_current = model_current.predict(X_current)\n",
    "                                current_r2 = r2_score(y_clean, y_pred_current)\n",
    "                                \n",
    "                                # Calculate incremental R²\n",
    "                                incremental_r2_value = current_r2 - previous_r2\n",
    "                                \n",
    "                                # Create incremental result\n",
    "                                incremental_result = {\n",
    "                                    'filename': filename,\n",
    "                                    'predictors_up_to': ', '.join(current_predictors),\n",
    "                                    'added_predictor': current_predictors[-1],\n",
    "                                    'outcome': outcome,\n",
    "                                    'n_samples': len(X_clean),\n",
    "                                    'n_predictors': i,\n",
    "                                    'intercept': model_current.intercept_,\n",
    "                                    'total_r2': current_r2,\n",
    "                                    'incremental_r2': incremental_r2_value,\n",
    "                                    'r2_change': incremental_r2_value,\n",
    "                                    'regression_type': 'incremental_multiple'\n",
    "                                }\n",
    "                                \n",
    "                                # Add coefficients for current model\n",
    "                                for j, pred in enumerate(current_predictors):\n",
    "                                    incremental_result[f'coef_{pred}'] = model_current.coef_[j]\n",
    "                                \n",
    "                                incremental_results.append(incremental_result)\n",
    "                                previous_r2 = current_r2\n",
    "                            \n",
    "                            # Add all incremental results\n",
    "                            results_list.extend(incremental_results)\n",
    "                            \n",
    "                        else:\n",
    "                            # Standard multiple regression (all predictors at once)\n",
    "                            model = LinearRegression()\n",
    "                            model.fit(X_clean, y_clean)\n",
    "                            y_pred = model.predict(X_clean)\n",
    "                            \n",
    "                            # Create result for multiple regression\n",
    "                            result = {\n",
    "                                'filename': filename,\n",
    "                                'predictors': ', '.join(available_predictors),\n",
    "                                'outcome': outcome,\n",
    "                                'n_samples': len(X_clean),\n",
    "                                'n_predictors': len(available_predictors),\n",
    "                                'intercept': model.intercept_,\n",
    "                                'r2_score': r2_score(y_clean, y_pred),\n",
    "                                'regression_type': 'multiple'\n",
    "                            }\n",
    "                            \n",
    "                            # Add individual coefficients\n",
    "                            for i, predictor in enumerate(available_predictors):\n",
    "                                result[f'coef_{predictor}'] = model.coef_[i]\n",
    "                            \n",
    "                            results_list.append(result)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error in multiple regression {available_predictors} -> {outcome}: {str(e)}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"  Running simple regression for {len(predictors)} predictors\")\n",
    "        \n",
    "        # Run simple regression for each predictor-outcome pair\n",
    "        for predictor in predictors:\n",
    "            for outcome in outcomes:\n",
    "                if predictor in data.columns and outcome in data.columns:\n",
    "                    try:\n",
    "                        X = data[[predictor]].values\n",
    "                        y = data[outcome].values\n",
    "                        \n",
    "                        # Remove any NaN values\n",
    "                        mask = ~(pd.isna(X).any(axis=1) | pd.isna(y))\n",
    "                        X_clean = X[mask]\n",
    "                        y_clean = y[mask]\n",
    "                        \n",
    "                        if len(X_clean) > 10:  # Minimum sample size\n",
    "                            model = LinearRegression()\n",
    "                            model.fit(X_clean, y_clean)\n",
    "                            y_pred = model.predict(X_clean)\n",
    "                            \n",
    "                            result = {\n",
    "                                'filename': filename,\n",
    "                                'predictors': predictor,\n",
    "                                'outcome': outcome,\n",
    "                                'n_samples': len(X_clean),\n",
    "                                'n_predictors': 1,\n",
    "                                'coefficient': model.coef_[0],\n",
    "                                'intercept': model.intercept_,\n",
    "                                'r2_score': r2_score(y_clean, y_pred),\n",
    "                                'regression_type': 'simple'\n",
    "                            }\n",
    "                            results_list.append(result)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error in simple regression {predictor} -> {outcome}: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(results_list) if results_list else None\n",
    "\n",
    "def correlation_analysis(data, filename=None, method='pearson', **kwargs):\n",
    "    \"\"\"\n",
    "    Correlation analysis between all numeric variables\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The data to analyze\n",
    "    filename (str): Optional filename for reference\n",
    "    method (str): Correlation method ('pearson', 'spearman', 'kendall')\n",
    "    **kwargs: Additional parameters\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Correlation matrix in long format\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get numeric columns only\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    if numeric_data.empty:\n",
    "        return pd.DataFrame({'filename': [filename], 'error': ['No numeric columns found']})\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = numeric_data.corr(method=method)\n",
    "    \n",
    "    # Convert to long format\n",
    "    results_list = []\n",
    "    for i, var1 in enumerate(corr_matrix.columns):\n",
    "        for j, var2 in enumerate(corr_matrix.columns):\n",
    "            if i < j:  # Only upper triangle to avoid duplicates\n",
    "                results_list.append({\n",
    "                    'filename': filename,\n",
    "                    'variable1': var1,\n",
    "                    'variable2': var2,\n",
    "                    'correlation': corr_matrix.loc[var1, var2],\n",
    "                    'method': method\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f517fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the directory analysis function\n",
    "def run_analysis_example():\n",
    "    \"\"\"\n",
    "    Example showing how to run various analyses on all files in a directory\n",
    "    \"\"\"\n",
    "    \n",
    "    # Specify the directory containing your data files\n",
    "    data_directory = \"/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Data/phenoVT_geneticAM/nfam8000\"\n",
    "    \n",
    "    # Output directory for results\n",
    "    output_directory = \"/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/phenoVT_geneticAM/nfam8000\"\n",
    "    \n",
    "    print(\"=== Running Basic Statistics on All Files ===\")\n",
    "    # Run basic statistics\n",
    "    stats_results = run_analysis_on_directory(\n",
    "        directory_path=data_directory,\n",
    "        analysis_function=basic_stats_analysis,\n",
    "        focal_trait='both',\n",
    "        combine_pgs=True,\n",
    "        file_pattern='*.txt',\n",
    "        save_results=True,\n",
    "        output_dir=output_directory\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nBasic statistics completed on {len(stats_results)} files\")\n",
    "    \n",
    "    # print(\"\\n=== Running Custom Regression Analysis (Multiple Regression) ===\")\n",
    "    # # Run multiple regression with specified predictors and outcomes\n",
    "    # multiple_regression_results = run_analysis_on_directory(\n",
    "    #     directory_path=data_directory,\n",
    "    #     analysis_function=custom_regression_analysis,\n",
    "    #     focal_trait='both',\n",
    "    #     combine_pgs=True,\n",
    "    #     file_pattern='*.txt',\n",
    "    #     save_results=True,\n",
    "    #     output_dir=output_directory,\n",
    "    #     # Additional parameters for the regression function\n",
    "    #     predictors=['PGSo1', 'PGSp1', 'PGSm1'],  # Multiple predictors for multiple regression\n",
    "    #     outcomes=['Yo1'],  # Single outcome\n",
    "    #     multiple_regression=True  # Enable multiple regression\n",
    "    # )\n",
    "    \n",
    "    # print(f\"\\nMultiple regression analysis completed on {len(multiple_regression_results)} files\")\n",
    "    \n",
    "    print(\"\\n=== Running Incremental R² Analysis ===\")\n",
    "    # Run incremental R² analysis to see contribution of each predictor\n",
    "    incremental_r2_results = run_analysis_on_directory(\n",
    "        directory_path=data_directory,\n",
    "        analysis_function=custom_regression_analysis,\n",
    "        focal_trait='both',\n",
    "        combine_pgs=True,\n",
    "        file_pattern='*.txt',\n",
    "        save_results=True,\n",
    "        output_dir=output_directory,\n",
    "        # Additional parameters for the regression function\n",
    "        predictors=['PGSo1', 'PGSp1', 'PGSm1'],  # Multiple predictors (order matters!)\n",
    "        outcomes=['Yo1'],  # Single outcome\n",
    "        multiple_regression=True,  # Enable multiple regression\n",
    "        incremental_r2=True  # Enable incremental R² analysis\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nIncremental R² analysis completed on {len(incremental_r2_results)} files\")\n",
    "    \n",
    "    # print(\"\\n=== Running Correlation Analysis ===\")\n",
    "    # # Run correlation analysis\n",
    "    # corr_results = run_analysis_on_directory(\n",
    "    #     directory_path=data_directory,\n",
    "    #     analysis_function=correlation_analysis,\n",
    "    #     focal_trait='both',\n",
    "    #     combine_pgs=False,  # Keep original columns for correlation\n",
    "    #     file_pattern='*.txt',\n",
    "    #     save_results=True,\n",
    "    #     output_dir=output_directory,\n",
    "    #     method='pearson'  # correlation method\n",
    "    # )\n",
    "    \n",
    "    # print(f\"\\nCorrelation analysis completed on {len(corr_results)} files\")\n",
    "    \n",
    "    return stats_results, multiple_regression_results, incremental_r2_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1ebc9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/tp/h_p12zs55v757md55fb928740000gp/T/ipykernel_31435/500284202.py:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def extract_key_results(analysis_results, key_columns=None, filename_pattern=None, \n",
    "                       pivot_columns=None, aggregate_func='first', \n",
    "                       include_metadata=True, sort_by=None):\n",
    "    \"\"\"\n",
    "    Extract specified key results from run_analysis_on_directory output and consolidate into a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    analysis_results (dict): Output from run_analysis_on_directory function\n",
    "                           Dictionary with filename as key and analysis results as value\n",
    "    key_columns (list): List of column names to extract from each analysis result\n",
    "                       If None, will extract all numeric columns\n",
    "                       Examples: ['r2_score', 'incremental_r2', 'total_r2']\n",
    "    filename_pattern (str): Optional regex pattern to filter filenames\n",
    "                           Example: r'run_(\\d+)' to extract run numbers\n",
    "    pivot_columns (list): Optional list of columns to pivot on\n",
    "                         Creates separate columns for each unique value\n",
    "                         Example: ['added_predictor'] creates columns for each predictor\n",
    "    aggregate_func (str or callable): How to aggregate multiple rows per file\n",
    "                                     Options: 'first', 'last', 'mean', 'max', 'min', 'sum'\n",
    "    include_metadata (bool): Whether to include metadata columns like filename, n_samples\n",
    "    sort_by (str or list): Column(s) to sort the final DataFrame by\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Consolidated DataFrame with rows as data files and columns as extracted metrics\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    if not analysis_results:\n",
    "        print(\"No analysis results provided\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    consolidated_data = []\n",
    "    \n",
    "    for filename, result in analysis_results.items():\n",
    "        try:\n",
    "            # Handle different types of results\n",
    "            if result is None:\n",
    "                continue\n",
    "                \n",
    "            # Convert to DataFrame if it's not already\n",
    "            if isinstance(result, dict):\n",
    "                result_df = pd.DataFrame([result])\n",
    "            elif isinstance(result, pd.DataFrame):\n",
    "                result_df = result.copy()\n",
    "            else:\n",
    "                print(f\"Warning: Unsupported result type {type(result)} for {filename}\")\n",
    "                continue\n",
    "            \n",
    "            if result_df.empty:\n",
    "                continue\n",
    "            \n",
    "            # Extract filename information\n",
    "            base_info = {'filename': filename}\n",
    "            \n",
    "            # Extract run number or other info from filename using pattern\n",
    "            if filename_pattern:\n",
    "                match = re.search(filename_pattern, filename)\n",
    "                if match:\n",
    "                    if match.groups():\n",
    "                        base_info['run_number'] = match.group(1)\n",
    "                    else:\n",
    "                        base_info['match'] = match.group(0)\n",
    "            \n",
    "            # Determine which columns to extract\n",
    "            if key_columns is None:\n",
    "                # Extract all numeric columns\n",
    "                numeric_cols = result_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                extract_cols = numeric_cols\n",
    "            else:\n",
    "                # Use specified columns that exist in the result\n",
    "                extract_cols = [col for col in key_columns if col in result_df.columns]\n",
    "            \n",
    "            # Include metadata columns if requested\n",
    "            metadata_cols = []\n",
    "            if include_metadata:\n",
    "                possible_metadata = ['n_samples', 'n_predictors', 'outcome', 'predictors', \n",
    "                                   'regression_type', 'method', 'added_predictor', \n",
    "                                   'variable1', 'variable2']\n",
    "                metadata_cols = [col for col in possible_metadata if col in result_df.columns]\n",
    "            \n",
    "            all_extract_cols = extract_cols + metadata_cols\n",
    "            \n",
    "            if not all_extract_cols:\n",
    "                print(f\"Warning: No columns to extract from {filename}\")\n",
    "                continue\n",
    "            \n",
    "            # Handle pivoting if requested\n",
    "            if pivot_columns and any(col in result_df.columns for col in pivot_columns):\n",
    "                print(f\"  Pivoting data for {filename}\")\n",
    "                \n",
    "                # Determine which pivot columns exist\n",
    "                existing_pivot_cols = [col for col in pivot_columns if col in result_df.columns]\n",
    "                \n",
    "                # Create a pivot table for each combination of pivot columns\n",
    "                if len(existing_pivot_cols) == 1:\n",
    "                    pivot_col = existing_pivot_cols[0]\n",
    "                    \n",
    "                    # For each numeric column, create separate columns for each pivot value\n",
    "                    pivoted_data = base_info.copy()\n",
    "                    \n",
    "                    for extract_col in extract_cols:\n",
    "                        if extract_col in result_df.columns:\n",
    "                            for pivot_value in result_df[pivot_col].unique():\n",
    "                                mask = result_df[pivot_col] == pivot_value\n",
    "                                subset = result_df[mask]\n",
    "                                \n",
    "                                if not subset.empty:\n",
    "                                    # Aggregate if multiple rows\n",
    "                                    if len(subset) > 1:\n",
    "                                        if aggregate_func == 'first':\n",
    "                                            value = subset[extract_col].iloc[0]\n",
    "                                        elif aggregate_func == 'last':\n",
    "                                            value = subset[extract_col].iloc[-1]\n",
    "                                        elif aggregate_func == 'mean':\n",
    "                                            value = subset[extract_col].mean()\n",
    "                                        elif aggregate_func == 'max':\n",
    "                                            value = subset[extract_col].max()\n",
    "                                        elif aggregate_func == 'min':\n",
    "                                            value = subset[extract_col].min()\n",
    "                                        elif aggregate_func == 'sum':\n",
    "                                            value = subset[extract_col].sum()\n",
    "                                        elif callable(aggregate_func):\n",
    "                                            value = aggregate_func(subset[extract_col])\n",
    "                                        else:\n",
    "                                            value = subset[extract_col].iloc[0]\n",
    "                                    else:\n",
    "                                        value = subset[extract_col].iloc[0]\n",
    "                                    \n",
    "                                    # Create column name\n",
    "                                    col_name = f\"{extract_col}_{pivot_value}\"\n",
    "                                    pivoted_data[col_name] = value\n",
    "                    \n",
    "                    # Add metadata (take first occurrence)\n",
    "                    for meta_col in metadata_cols:\n",
    "                        if meta_col not in pivot_columns and meta_col in result_df.columns:\n",
    "                            pivoted_data[meta_col] = result_df[meta_col].iloc[0]\n",
    "                    \n",
    "                    consolidated_data.append(pivoted_data)\n",
    "                \n",
    "                else:\n",
    "                    print(f\"Warning: Multiple pivot columns not yet supported for {filename}\")\n",
    "                    continue\n",
    "            \n",
    "            else:\n",
    "                # No pivoting - handle multiple rows by aggregating\n",
    "                if len(result_df) > 1:\n",
    "                    print(f\"  Aggregating {len(result_df)} rows for {filename}\")\n",
    "                    \n",
    "                    row_data = base_info.copy()\n",
    "                    \n",
    "                    # Aggregate numeric columns\n",
    "                    for col in extract_cols:\n",
    "                        if col in result_df.columns:\n",
    "                            if aggregate_func == 'first':\n",
    "                                row_data[col] = result_df[col].iloc[0]\n",
    "                            elif aggregate_func == 'last':\n",
    "                                row_data[col] = result_df[col].iloc[-1]\n",
    "                            elif aggregate_func == 'mean':\n",
    "                                row_data[col] = result_df[col].mean()\n",
    "                            elif aggregate_func == 'max':\n",
    "                                row_data[col] = result_df[col].max()\n",
    "                            elif aggregate_func == 'min':\n",
    "                                row_data[col] = result_df[col].min()\n",
    "                            elif aggregate_func == 'sum':\n",
    "                                row_data[col] = result_df[col].sum()\n",
    "                            elif callable(aggregate_func):\n",
    "                                row_data[col] = aggregate_func(result_df[col])\n",
    "                            else:\n",
    "                                row_data[col] = result_df[col].iloc[0]\n",
    "                    \n",
    "                    # Handle metadata columns\n",
    "                    for col in metadata_cols:\n",
    "                        if col in result_df.columns:\n",
    "                            # For metadata, usually take first value or most common\n",
    "                            if result_df[col].dtype == 'object':\n",
    "                                row_data[col] = result_df[col].iloc[0]  # Take first for text\n",
    "                            else:\n",
    "                                row_data[col] = result_df[col].iloc[0]  # Take first for numbers\n",
    "                    \n",
    "                    consolidated_data.append(row_data)\n",
    "                \n",
    "                else:\n",
    "                    # Single row - just extract the values\n",
    "                    row_data = base_info.copy()\n",
    "                    \n",
    "                    for col in all_extract_cols:\n",
    "                        if col in result_df.columns:\n",
    "                            row_data[col] = result_df[col].iloc[0]\n",
    "                    \n",
    "                    consolidated_data.append(row_data)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Create final DataFrame\n",
    "    if not consolidated_data:\n",
    "        print(\"No data was successfully extracted\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    final_df = pd.DataFrame(consolidated_data)\n",
    "    \n",
    "    # Sort if requested\n",
    "    if sort_by and sort_by in final_df.columns:\n",
    "        final_df = final_df.sort_values(sort_by).reset_index(drop=True)\n",
    "    elif isinstance(sort_by, list):\n",
    "        available_sort_cols = [col for col in sort_by if col in final_df.columns]\n",
    "        if available_sort_cols:\n",
    "            final_df = final_df.sort_values(available_sort_cols).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Successfully extracted data from {len(consolidated_data)} files\")\n",
    "    print(f\"Final DataFrame shape: {final_df.shape}\")\n",
    "    print(f\"Columns: {list(final_df.columns)}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def extract_r2_results(analysis_results, r2_types=None, filename_pattern=r'run_(\\d+)', \n",
    "                      include_predictors=True, sort_by_run=True):\n",
    "    \"\"\"\n",
    "    Specialized function to extract R² values from regression analysis results.\n",
    "    \n",
    "    Parameters:\n",
    "    analysis_results (dict): Output from run_analysis_on_directory function\n",
    "    r2_types (list): List of R² column names to extract\n",
    "                    If None, will detect all R² columns automatically\n",
    "                    Examples: ['r2_score', 'total_r2', 'incremental_r2']\n",
    "    filename_pattern (str): Regex pattern to extract run numbers from filenames\n",
    "    include_predictors (bool): Whether to include predictor information\n",
    "    sort_by_run (bool): Whether to sort by run number\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with runs as rows and R² values as columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Auto-detect R² column types if not specified\n",
    "    if r2_types is None:\n",
    "        r2_types = []\n",
    "        # Check a sample of results to find R² columns\n",
    "        for result in analysis_results.values():\n",
    "            if result is not None:\n",
    "                if isinstance(result, dict):\n",
    "                    sample_df = pd.DataFrame([result])\n",
    "                elif isinstance(result, pd.DataFrame):\n",
    "                    sample_df = result\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Find columns that likely contain R² values\n",
    "                potential_r2_cols = [col for col in sample_df.columns \n",
    "                                   if 'r2' in col.lower() or 'r_squared' in col.lower()]\n",
    "                r2_types.extend(potential_r2_cols)\n",
    "        \n",
    "        # Remove duplicates and sort\n",
    "        r2_types = sorted(list(set(r2_types)))\n",
    "        print(f\"Auto-detected R² columns: {r2_types}\")\n",
    "    \n",
    "    # Determine what to include based on the type of analysis\n",
    "    key_columns = r2_types.copy()\n",
    "    \n",
    "    # Add predictor information if requested\n",
    "    if include_predictors:\n",
    "        key_columns.extend(['predictors', 'added_predictor', 'outcome'])\n",
    "    \n",
    "    # Check if we need to pivot (for incremental R² results)\n",
    "    needs_pivot = any('incremental' in col.lower() for col in r2_types)\n",
    "    pivot_columns = ['added_predictor'] if needs_pivot else None\n",
    "    \n",
    "    # Extract the results\n",
    "    r2_df = extract_key_results(\n",
    "        analysis_results=analysis_results,\n",
    "        key_columns=key_columns,\n",
    "        filename_pattern=filename_pattern,\n",
    "        pivot_columns=pivot_columns,\n",
    "        aggregate_func='first',  # Take first occurrence\n",
    "        include_metadata=True,\n",
    "        sort_by='run_number' if sort_by_run else None\n",
    "    )\n",
    "    \n",
    "    return r2_df\n",
    "\n",
    "\n",
    "# Example usage functions\n",
    "def demo_extract_key_results(condition = None):\n",
    "    \"\"\"\n",
    "    Demonstrate how to use the extract_key_results function\n",
    "    \"\"\"\n",
    "    print(\"=== Demo: Extract Key Results Function ===\")\n",
    "    \n",
    "    # First, run some analysis to get results\n",
    "    data_directory = f\"/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Data/{condition}/nfam8000\"\n",
    "    \n",
    "    print(\"1. Running incremental R² analysis on a few files...\")\n",
    "    # Run analysis on just a few files for demo\n",
    "    import glob\n",
    "    files = glob.glob(os.path.join(data_directory, \"*.txt\"))  # Just first 5 files\n",
    "    \n",
    "    demo_results = {}\n",
    "    for file_path in files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"  Processing {filename}\")\n",
    "        \n",
    "        try:\n",
    "            data = read_trait_data(file_path, focal_trait='both', combine_pgs=True)\n",
    "            if data is not None:\n",
    "                result = custom_regression_analysis(\n",
    "                    data, \n",
    "                    filename=filename,\n",
    "                    predictors=['PGSo2', 'PGSp2', 'PGSm2'],\n",
    "                    outcomes=['Yo2'],\n",
    "                    multiple_regression=True,\n",
    "                    incremental_r2=True\n",
    "                )\n",
    "                demo_results[filename] = result\n",
    "        except Exception as e:\n",
    "            print(f\"    Error: {e}\")\n",
    "    \n",
    "    print(f\"\\n2. Got results from {len(demo_results)} files\")\n",
    "    \n",
    "    # Now demonstrate extracting key results\n",
    "    print(\"\\n3. Extracting R² values using specialized function...\")\n",
    "    r2_summary = extract_r2_results(\n",
    "        demo_results,\n",
    "        r2_types=['total_r2', 'incremental_r2'],\n",
    "        filename_pattern=r'run_(\\d+)',\n",
    "        include_predictors=True,\n",
    "        sort_by_run=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nR² Summary DataFrame:\")\n",
    "    print(r2_summary)\n",
    "    \n",
    "    # # Demonstrate generic extraction\n",
    "    # print(\"\\n4. Extracting custom columns using generic function...\")\n",
    "    # custom_summary = extract_key_results(\n",
    "    #     demo_results,\n",
    "    #     key_columns=['total_r2', 'incremental_r2', 'n_samples'],\n",
    "    #     filename_pattern=r'run_(\\d+)',\n",
    "    #     pivot_columns=['added_predictor'],\n",
    "    #     include_metadata=True,\n",
    "    #     sort_by='run_number'\n",
    "    # )\n",
    "    \n",
    "    # print(\"\\nCustom Summary DataFrame:\")\n",
    "    # print(custom_summary)\n",
    "    \n",
    "    return demo_results, r2_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fec6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for one condition\n",
    "test = demo_extract_key_results(condition='phenoVT_geneticAM')\n",
    "\n",
    "# save the results to a csv file\n",
    "test[1].to_csv('/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/phenoVT_geneticAM_trait2_r2_summary.csv', index=False)\n",
    "\n",
    "# for second condition\n",
    "test2 = demo_extract_key_results(condition='phenoVT_socialAM')\n",
    "# save the results to a csv file\n",
    "test2[1].to_csv('/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/phenoVT_socialAM_trait2_r2_summary.csv', index=False)\n",
    "\n",
    "# third condition\n",
    "test3 = demo_extract_key_results(condition='phenoVT_phenoAM')\n",
    "# save the results to a csv file\n",
    "test3[1].to_csv('/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/phenoVT_phenoAM_trait2_r2_summary.csv', index=False)\n",
    "\n",
    "# fourth condition\n",
    "test4 = demo_extract_key_results(condition='socialVT_phenoAM')\n",
    "# save the results to a csv file\n",
    "test4[1].to_csv('/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/socialVT_phenoAM_trait2_r2_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a002aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the trait1 results\n",
    "# read the saved csv file\n",
    "\n",
    "df_phenoVT_geneticAM = pd.read_csv('/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/phenoVT_geneticAM_trait1_r2_summary.csv')\n",
    "df_phenoVT_socialAM = pd.read_csv('/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/phenoVT_socialAM_trait1_r2_summary.csv')\n",
    "df_phenoVT_phenoAM = pd.read_csv('/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/phenoVT_phenoAM_trait1_r2_summary.csv')\n",
    "df_socialVT_phenoAM = pd.read_csv('/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/socialVT_phenoAM_trait1_r2_summary.csv')\n",
    "\n",
    "# add a condition column to each dataframe and rearrange it to be the first column\n",
    "df_phenoVT_geneticAM['condition'] = 'phenoVT_geneticAM'\n",
    "df_phenoVT_socialAM['condition'] = 'phenoVT_socialAM'\n",
    "df_phenoVT_phenoAM['condition'] = 'phenoVT_phenoAM'\n",
    "df_socialVT_phenoAM['condition'] = 'socialVT_phenoAM'\n",
    "# concatenate the dataframes\n",
    "df_combined = pd.concat([df_phenoVT_geneticAM, df_phenoVT_socialAM, df_phenoVT_phenoAM, df_socialVT_phenoAM], ignore_index=True)\n",
    "\n",
    "# get summary statistics for the total_r2_PGSo1,total_r2_PGSp1,total_r2_PGSm1,incremental_r2_PGSo1,incremental_r2_PGSp1,incremental_r2_PGSm1\n",
    "# by condition, include mean, std, min, max, median, MAD\n",
    "# Get summary statistics for specified R² columns by condition\n",
    "target_columns = ['total_r2_PGSo1', 'total_r2_PGSp1', 'total_r2_PGSm1', \n",
    "                  'incremental_r2_PGSo1', 'incremental_r2_PGSp1', 'incremental_r2_PGSm1']\n",
    "\n",
    "# Create long format summary table\n",
    "summary_list = []\n",
    "for condition in df_combined['condition'].unique():\n",
    "    condition_data = df_combined[df_combined['condition'] == condition]\n",
    "    for col in target_columns:\n",
    "        values = condition_data[col].dropna()\n",
    "        if len(values) > 0:\n",
    "            summary_list.append({\n",
    "                'condition': condition,\n",
    "                'variable': col,\n",
    "                'count': len(values),\n",
    "                'mean': values.mean(),\n",
    "                'std': values.std(),\n",
    "                'min': values.min(),\n",
    "                'max': values.max(),\n",
    "                'median': values.median(),\n",
    "                'mad': np.median(np.abs(values - values.median()))\n",
    "            })\n",
    "\n",
    "summary_stats = pd.DataFrame(summary_list).round(4)\n",
    "\n",
    "# write the summary statistics to a tsv file\n",
    "summary_stats.to_csv('/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/regression_trait1_r2_summary_stats.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b5b9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the trait2 results\n",
    "df_phenoVT_geneticAM_trait2 = pd.read_csv('/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/phenoVT_geneticAM_trait2_r2_summary.csv')\n",
    "df_phenoVT_socialAM_trait2 = pd.read_csv('/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/phenoVT_socialAM_trait2_r2_summary.csv')\n",
    "df_phenoVT_phenoAM_trait2 = pd.read_csv('/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/phenoVT_phenoAM_trait2_r2_summary.csv')\n",
    "df_socialVT_phenoAM_trait2 = pd.read_csv('/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/socialVT_phenoAM_trait2_r2_summary.csv')\n",
    "df_phenoVT_geneticAM_trait2['condition'] = 'phenoVT_geneticAM'\n",
    "df_phenoVT_socialAM_trait2['condition'] = 'phenoVT_socialAM'\n",
    "df_phenoVT_phenoAM_trait2['condition'] = 'phenoVT_phenoAM'\n",
    "df_socialVT_phenoAM_trait2['condition'] = 'socialVT_phenoAM'\n",
    "# concatenate the dataframes\n",
    "df_combined_trait2 = pd.concat([df_phenoVT_geneticAM_trait2, df_phenoVT_socialAM_trait2, \n",
    "                                 df_phenoVT_phenoAM_trait2, df_socialVT_phenoAM_trait2], ignore_index=True)\n",
    "# Get summary statistics for specified R² columns by condition\n",
    "target_columns_trait2 = ['total_r2_PGSo2', 'total_r2_PGSp2', 'total_r2_PGSm2', \n",
    "                         'incremental_r2_PGSo2', 'incremental_r2_PGSp2', 'incremental_r2_PGSm2']\n",
    "\n",
    "# Create long format summary table\n",
    "summary_list_trait2 = []\n",
    "for condition in df_combined_trait2['condition'].unique():\n",
    "    condition_data = df_combined_trait2[df_combined_trait2['condition'] == condition]\n",
    "    for col in target_columns_trait2:\n",
    "        values = condition_data[col].dropna()\n",
    "        if len(values) > 0:\n",
    "            summary_list_trait2.append({\n",
    "                'condition': condition,\n",
    "                'variable': col,\n",
    "                'count': len(values),\n",
    "                'mean': values.mean(),\n",
    "                'std': values.std(),\n",
    "                'min': values.min(),\n",
    "                'max': values.max(),\n",
    "                'median': values.median(),\n",
    "                'mad': np.median(np.abs(values - values.median()))\n",
    "            })\n",
    "\n",
    "summary_stats_trait2 = pd.DataFrame(summary_list_trait2).round(4)\n",
    "# write the summary statistics to a tsv file\n",
    "summary_stats_trait2.to_csv('/Users/xuly4739/Library/CloudStorage/OneDrive-UCB-O365/Documents/coding/PyProject/StatRev_IndirectGene/Analysis/PGS-Regression/results/regression_trait2_r2_summary_stats.tsv', sep='\\t')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
